## 概要
S3のバックアップをEC2に保存していることによってEC2のディスクアラートが発生している。
→ これをやめたい
EC2のディスクは49GB(多分)くらいあるはず

## 対応・調査内容
- [x] 松本さんが以前調べているので、その時の結果・概要を聞いておく
  - `bee2b-infra`チャンネルで確認済み

#### EC2ディスク(EBS)
- アプリケーション以外で何に使ってる？
49GBの内訳を知りたい。
ログ何GB、バックアップ何GBなど
- バックアップが多い場合→バックアップ用のバケットに移すだけ
- ログが多い場合→ライフサイクルを決めて対応しなければならないため、内訳が必要


一時的な対応
- S3のバックアップをEC2に移動しているが、S3の別のバケットに移動
恒久的な対応
- S3のバックアップ用バケットを作成し、堺、鹿島、水島に横展開していく。
- いつ対応するのかなどはbacklogのアカウントを作成してもらってから追記する形で良い。



## 自身の理解
- S3のバックアップをなぜかEBSにコピーしている


## 調査結果
![スクリーンショット 2025-03-17 14.20.51.png](../_resources/スクリーンショット%202025-03-17%2014.20.51.png)
![スクリーンショット 2025-03-17 14.03.29.png](../_resources/スクリーンショット%202025-03-17%2014.03.29.png)
- 全体容量：100GB
- 全体使用量（ほぼ`/`）：70GB / 100GB
  - `/home/ec2-user`容量：33GB / 70GB
    - `app`容量（ログ込み）：7GB / 33GB
    - `s3`容量（S3バックアップ先）：4GB / 33GB
	- `dump`容量（SQLDump保存先）：21GB / 33GB

## 懸念点
cronでコピー対象となっているS3バケットの `ene-prd-app`は合計容量 50GBほどある。それに対して `/home/ec2-user/s3`は4GBしか容量がない。
→コピー中に容量がオーバーしてロールバックが発生している？

一旦S3からのバックアップが原因でしょうということで解決




S3のバックアップをEC2に保存している
→ これをやめて欲しい
EC2のディスクアラート

松本さんに調べた結果を聞く

49GBくらい（多分）
アプリケーション以外で何に使ってる？

### 調査内容
49GBの内訳を知りたい。
ログ何GB、バックアップ何GBなど
- バックアップが多い場合→バックアップ用のバケットに移すだけ
- ログが多い場合→ライフサイクルを決めて対応しなければならないため、内訳が必要


一時的な対応
- S3のバックアップをEC2に移動しているが、S3の別のバケットに移動
恒久的な対応
- S3のバックアップ用バケットを作成し、堺、鹿島、水島に横展開していく。
- いつ対応するのかなどはbacklogのアカウントを作成してもらってから追記する形で良い。


# 「3/25 インフラ」ノート統合
## 前提
S3のバックアップをEC2に移行しており、nginxのログなどで容量を圧迫している。
ライフサイクルを決めて

## 進め方
- VQに要件を確認
	- BUの対象
	- 復元の有無
### 対象
- DBのDUMP
- S3のバックアップ

## STEP1
#### まとめること
対象：DUMP、D3
- [ ] 現状はどれくらいの容量で、1日の増加量はどれくらいか
- [ ] 何日くらい保持するのか
nginx...定刻で容量の推移を確認し、（毎日何時）
ログローテーションの設定が必要か？
- [ ] 過去1ヶ月のアクセス容量がどれくらいの容量か確認。

現状の整理（明日中）
- 現状
- 対象
- 大雑把な対応
	- 

## STEP2
### S3
AWS backup
山下さんに設計の叩きを作ってほしい。

p1


p2
別のバケットに対比させて

### DUMP
S3に保存。ライフサイクルを設定。
提案の叩き台を山下さんに

- S3で管理だと日時で作成。
	- ライフサイクル
 
### nginx
S3に保存

## 木曜日の週次で報告
- [x] 資料作成
	- [x] ひとつの資料にまとめる
	- [x] jsonについて調べる
	- [x] S3の容量調べる


## 3/28 追加調査
- nginxのjsonと普通のやつの中身を確認して、同じならjson消す。
- [x] json消したいからその証拠出してほしい
json見る時はcat使わないで。tailで他のファイルに出したり
ダウンロードしてローカルで試すのがいいのでは？sftp scp
`mkdir task.json`
`tail -n 3000 > task.json/access.log.json_3000`
### json調査結果
- 黄色
	- `host`：`192.168.121.179`
	- `vhost`：`192.168.112.120`
	- `status`：`200`
	- `protocol`：`HTTP/1.1`
	- `request`：`GET /403.html HTTP/1.1`
	- `size`：`735`

- RDSのスナップショット使おうとなっている。

- [x] applicatoin.logを項目として増やす。概要とスクリーンショット記載する。

- [x] s3はzipになっているのか